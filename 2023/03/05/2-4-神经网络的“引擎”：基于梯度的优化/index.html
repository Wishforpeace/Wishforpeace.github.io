<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    <meta name="description" content="Hexo Theme Redefine">
    <meta name="author" content="The Redefine Team">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-redefine.png">
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/03/05/2-4-神经网络的“引擎”：基于梯度的优化/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    <meta property="og:type" content="article">
    <meta property="og:title" content="2.4　神经网络的“引擎”：基于梯度的优化">
    <meta property="og:description" content="Hexo Theme Redefine">
    <meta property="og:url" content="http://example.com2023/03/05/2-4-神经网络的“引擎”：基于梯度的优化/">
    
    <meta property="og:site_name" content="Theme Redefine">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="2.4　神经网络的“引擎”：基于梯度的优化">
    <meta name="twitter:description" content="Hexo Theme Redefine">
    <meta name="twitter:image" content="/images/redefine-logo.svg">
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-logo.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-logo.svg">
    <meta name="theme-color" content="#005080">
    <link rel="shortcut icon" href="/images/redefine-logo.svg">
    
    <title>
        
            2.4　神经网络的“引擎”：基于梯度的优化 -
        
        Theme Redefine
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    
    
    
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"example.com","root":"/","language":"en"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/redefine-avatar.svg","favicon":"/images/redefine-logo.svg","og_image":{"enable":false,"image_url":null},"article_img_align":"center","right_side_width":"210px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"Theme Redefine","custom_font":{"enable":false,"font_family":null,"font_url":null}},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":false,"preload":true},"code_block":{"copy":true,"style":"mac","custom_font":{"enable":false,"font_family":null,"font_url":null}},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"1.2.0","friend_links":{"columns":2},"home_article":{"date_format":"auto","category":{"enable":true,"limit":3},"tag":{"enable":true,"limit":3}},"plugins":{"aplayer":{"enable":false,"audio":[{"name":null,"artist":null,"url":null,"cover":null},{"name":null,"artist":null,"url":null,"cover":null}]}}};
    REDEFINE.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
    
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="menu-wrapper">
    
    <div class="menu-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Theme Redefine
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="pc">
                <ul class="menu-list">
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="menu-drawer">
        <ul class="drawer-menu-list">
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">2.4　神经网络的“引擎”：基于梯度的优化</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/redefine-avatar.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">The Redefine Team</span>
                            
                                <span class="author-label">lol</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="pc">2023-03-05 21:28:54</span>
        <span class="mobile">2023-03-05 21:28</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="pc">2023-03-10 09:18</span>
            <span class="mobile">2023-03-10 09</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Deep-Learning/">Deep Learning</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="神经网络的引擎基于梯度的优化">2-4
神经网络的“引擎”：基于梯度的优化</h1>
<p>随机初始化：选择权重较小的矩阵进行运算。</p>
<p>训练：逐步调节</p>
<p><strong>训练循环</strong>：</p>
<ul>
<li>抽取训练样本x和对应目标y_true组成的一个数据批量</li>
<li>在x上运行模型，得到y_pred（向前传播）</li>
<li>计算损失值，用于衡量y_pred和y_true之间的差距</li>
<li>更新模型的所有权重，以略微减小模型在这批数据上的损失值。</li>
</ul>
<h2 id="导数">2.4.1 导数</h2>
<p><span class="math display">\[f(x) = y\]</span></p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230305213325111.png"
                     
alt="image-20230305213325111" 
                >
<figcaption aria-hidden="true">image-20230305213325111</figcaption>
</figure>
<p>函数是连续的，x增加很小的epsilon,y也发生微小变化。</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230305213445742.png"
                     
alt="image-20230305213445742" 
                >
<figcaption aria-hidden="true">image-20230305213445742</figcaption>
</figure>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x + epsilon_x) = y + a * epsilon_x</span><br></pre></td></tr></table></figure></div>
<p>x足够接近p时，近似认为是斜率</p>
<p>斜率a时f在p点的导数</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230305213623201.png"
                     
alt="image-20230305213623201" 
                >
<figcaption aria-hidden="true">image-20230305213623201</figcaption>
</figure>
<h2 id="张量运算的导数梯度">2.4.2 张量运算的导数：梯度</h2>
<p>标量元组<code>(x,y)</code>映射一个标量值<code>z</code>，可以绘制三维空间（以x、y、z为坐标轴）中的二维表面。</p>
<p>张量运算的导数叫做梯度（gradient）.</p>
<p>对于标量函数，导数表示曲线的局部斜率(local
slope)是加入了epsilon,epsilon足够小时，接近斜率。</p>
<p><strong>张量函数的梯度表示该函数所对应多维表面的曲率(curvature)。</strong></p>
<p>以机器学习中的一个例子为展示：</p>
<ul>
<li>输入向量x（数据集的一个样本）</li>
<li>矩阵W(模型权重)</li>
<li>目标值y_true(模型应该学到的与x相关的结果)</li>
<li>一个损失函数loss(用于衡量模型当前预测值与y_true之间的差距)</li>
</ul>
<p>可以用W来计算预测值y_pre，然后计算损失值，即预测值y_pre与目标值y_true之间的差距</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = dot(W,x) <span class="comment">#利用模型权重w对x进行预测</span></span><br><span class="line">loss_value = loss(y_pred,y_true) <span class="comment">#估算预测值的偏差有多大</span></span><br></pre></td></tr></table></figure></div>
<p><strong>用梯度更新W，以使loss_value变小</strong></p>
<p>保持输入数据x和y_true不变，可以讲前面的运算看作一个将模型权重W的值映射到损失值的函数</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_value = f(w) <span class="comment">#f描述的是：当W变化时，损失值所形成的曲线（或高维表面）</span></span><br></pre></td></tr></table></figure></div>
<p>假设W的当前值为<code>W0</code>，f在W0点的导数是一个张量<code>grad(loss_value,W0)</code></p>
<p>其形状与W相同，每个元素<code>grad(loss_value,W0)[i][j]</code>表示当<code>W0[i][j]</code>发生变化时<code>loss_value</code>变化的方向和大小。张量<code>grad(loss_value,W0)</code>函数<code>f(w)=loss_value</code>在W0处的梯度，也叫做loss_value相对于W在W0附近的梯度。</p>
<h3 id="偏导数">偏导数</h3>
<p>张量运算<code>grad(f(W),W)</code>以矩阵W为输入，它可以表示为<strong>标量函数<code>grad_ij(f(w),w_ij)</code>的组合</strong>，每个标量函数返回的是<code>loss_value = f(W)</code>相对于<code>W[i,j]</code>的导数。（假设W的其他所有元素都不变）</p>
<p><code>grad_ij</code>叫作f对于<code>W[i,j]</code>的偏导数。(partial
derivative)。</p>
<p><code>grad(loss_value,W0)</code><strong>具体含义</strong>：</p>
<p>单变量函数f(x)的导数可以看做函数f曲线的斜率，同样的，<code>grad(loss_value,W0)</code>可以看作表示<code>loss_value = f(W)</code>在W0附近最陡上升方向的张量，也表示这一上升方向的斜率。<strong>每个偏导数表示f在某个方向的斜率。</strong></p>
<p>对于一个函数f(x)沿着导数的反方向移动可以进一步减小f(x)的值，同样对于一个张量f(W)，可以将W沿着梯度的反方向移动来减小<code>loss_value = f(W)</code></p>
<p>比如：</p>
<p>W1 = W0 - step * grad(f(W0),W0)</p>
<p>step是一个很小的比例因子。</p>
<p>沿着f最陡上升的反方向移动，直观上可以移动到曲线上更低的位置。</p>
<p><strong>注意</strong>：比例因子step是必须的，因为grad(loss_value,W0)只是W0附近曲率的近似值，所以不能离W0太远。</p>
<h2 id="随机梯度下降">2.4.3 随机梯度下降</h2>
<p>可微函数的最小值在导数为0处</p>
<p>应用于神经网络之中，通过对<code>grad(f(W),W) = 0</code>求解W来实现。</p>
<p>但是在实际神经网络中无法求解，参数的个数成千上万个。</p>
<h3
id="小批量随机梯度下降小批量sgd">小批量随机梯度下降（小批量SGD）</h3>
<blockquote>
<p>stochastic = random</p>
</blockquote>
<ol type="1">
<li>抽取训练样本x和对应目标的y_true组成一个数据批量</li>
<li>在x上运行模型，得到预测值y_pred（向前传播）</li>
<li>计算损失值，衡量y_pred和y_true之间的差距</li>
<li>计算损失相对于模型参数的梯度(反向传播)</li>
<li>将参数沿着梯度的反方向移动一小步，比如W-=learning_rate *
gradient，从而使这批数据上的损失值减小一点。学习率（learning_rate)是一个调节梯度下降速度的标量因子。</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307161415582.png"
                      alt="沿着一维损失函数曲线的随机梯度下降" style="zoom:50%;" 
                ></p>
<p>小批量SGD算法的一个变体每次迭代只选取一个样本和目标，而不是抽取一批数据。</p>
<p>这是<strong>真SGD</strong>，有别于小批量SGD。</p>
<p>另一种极端，<strong>批量梯度下降</strong>，每次迭代在所有数据上。</p>
<h3 id="sgd的多种变体">SGD的多种变体</h3>
<ul>
<li>带动量的SGD</li>
<li>Adagrad</li>
<li>RMSprop</li>
</ul>
<p>这些变体在计算下一次权重更新时，不仅要考虑当前的梯度值，还要考虑上一次权重的更新，这些方法被称为优化方法（optimization
method）或者优化器（optimizer）。</p>
<h3 id="动量sgd">动量SGD</h3>
<p>解决了两个问题：</p>
<ul>
<li>收敛速度</li>
<li>局部极小值</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307162333831.png"
                      alt="局部极小点和全局极小点" style="zoom:50%;" 
                ></p>
<p>在图中局部极小点处，左移和右移都会导致损失值的增大，所以使用learning_rate较小的SGD对参数进行优化，优化过程会陷入到局部极小点，而无法找到全局极小点。</p>
<p>使用动量方法，参考物理，每次移动小球，不仅考虑它的当前加速度，还考虑它的速度。</p>
<p><strong>实践意义</strong>：更新W时不仅考虑当前的梯度值，还要考虑上一次参数更新。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">past_velocity = <span class="number">0.</span></span><br><span class="line">momentum = <span class="number">0.1</span>   ←---- 不变的动量因子</span><br><span class="line"><span class="keyword">while</span> loss &gt; <span class="number">0.01</span>:   ←---- 优化循环</span><br><span class="line">    w, loss, gradient = get_current_parameters()</span><br><span class="line">    velocity = past_velocity * momentum - learning_rate * gradient</span><br><span class="line">    w = w + momentum * velocity - learning_rate * gradient</span><br><span class="line">    past_velocity = velocity</span><br><span class="line">    update_parameter(w)</span><br></pre></td></tr></table></figure></div>
<h2 id="链式求导反向传播算法">2.4.4 链式求导：反向传播算法</h2>
<p>双层模型，计算损失相对于权重的梯度，使用<strong>反向传播算法</strong>。</p>
<h3 id="链式法则">01.链式法则</h3>
<p>利用简单运算的导数，可以轻松算出这些基本运算的任意复杂组合的梯度。</p>
<p>神经网络由许多链接在一起的张量运算组成，每个张量运算的的导数已知，且都很简单。</p>
<p>代码清单2-2中定义的模型，一个由变量W1、b1、W2、b2（分别属于第一个Dense层和第二个Dense层）参数化的函数，运用到的基本运算是dot、relu、sotfmax和+，以及损失函数loss。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_value = loss(y_true, softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))</span><br></pre></td></tr></table></figure></div>
<h4 id="链式法则求导chain-rule">链式法则求导（Chain Rule）</h4>
<p>两个函数f和g复合。</p>
<p>复合函数：<span class="math display">\[fg:fg(x)==f(g(x))\]</span></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fg</span>(<span class="params">x</span>):</span><br><span class="line">  x1 = g(x)</span><br><span class="line">  y = f(x1)</span><br><span class="line">  <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure></div>
<p>链式法则规定：<span class="math display">\[grad(y,x) = grad(y,x1) *
grad(x1,x)\]</span></p>
<p>知道f和g的导数，就能求出fg的导数，添加多个函数就像一条链，因此称为链式法则。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fghj</span>:</span><br><span class="line">  x1 = j(x)</span><br><span class="line">  x2 = g(x1)</span><br><span class="line">  x3 = h(x2)</span><br><span class="line">  y = f(x3)</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line">grad(y,x) = (grad(y,x3),grad(x3,x2),grad(x2,x1),grad(x1,x))</span><br></pre></td></tr></table></figure></div>
<p>将链式法则应用于神经网络梯度值的计算，就得到了一种叫作<strong>反向传播</strong>的算法。</p>
<h3 id="用计算图自动划分">02 用计算图自动划分</h3>
<p>计算图是思考反向传播算法的一种有用方法，深度学习的核心数据结构，由运算构成的有向无环图。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307171139764.png"
                      alt="双层模型示例的计算图表示" style="zoom: 33%;" 
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307171340596.png"
                      alt="简单计算图示例" style="zoom:33%;" 
                ></p>
<p>取两个标量变量 w 和 b，以及一个标量输入
x，然后对它们做一些运算，得到输出
y。最后，我们使用绝对值误差损失函数：<span
class="math display">\[loss\underline{ }val = abs(y\underline{ }true -
y)\]</span>。我们希望更新<span class="math display">\[ w \]</span>和
<span class="math display">\[b \]</span>以使<span
class="math display">\[ loss\underline{ }val
\]</span>最小化，所以需要计算 <span
class="math display">\[grad(loss\underline{ }val, b)\]</span> 和 <span
class="math display">\[grad(loss\underline{ }val, w)\]</span>。</p>
<p>从上到下，直到获得<span class="math display">\[loss\underline{
}value\]</span>，这是<strong>向前传播</strong>的过程。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307171820440.png"
                      alt="运行一次向前传播" style="zoom:33%;" 
                ></p>
<p>画出反向的边，表示<strong>反向传播</strong>的过程。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307171935175.png"
                      alt="运行一次反向传播" style="zoom:33%;" 
                ></p>
<p><strong>结果</strong>：</p>
<ul>
<li>grad(loss_val, x2) = 1，随x2变化一个小量epsilon, loss_val = abs(4 -
x2) 的变化量相同。</li>
<li>grad(x2, x1) = 1，随着x1变化一个小量epsilon，x2 = x1 + b = x1 + 1
的变化量相同。</li>
<li>grad(x2, b) = 1，随着b变化一个小量epsilon, x2 = x1 +b = 6+
b的变化量相同。</li>
<li>grad(x1, w) = 2，随着w变化一个小量epsilon，x1 = x * w = 2 *
w的变化量为2倍的epsilon。</li>
</ul>
<p>根据链式法则，对于反向图，想求一个节点相对于另一个节点的导数，将链接两个节点的路径上的每条边的导数相乘。</p>
<p>比如计算grad(loss, w) = grad(loss, x2) * grad(x2, x1) * grad(x1,
w)</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Hyeonwuu/Image/image-20230307173444116.png"
                      alt="在反向图中从 loss_val 到 w 的路径" style="zoom:33%;" 
                ></p>
<p>对应链式法则，得到想要的结果：</p>
<ul>
<li>grad(loss_val, w) = 1 * 1 * 2 = 2</li>
<li>Grad()</li>
</ul>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>Post title：2.4　神经网络的“引擎”：基于梯度的优化</li>
        <li>Post author：The Redefine Team</li>
        <li>Create time：2023-03-05 21:28:54</li>
        <li>
            Post link：https://redefine.ohevan.com/2023/03/05/2-4-神经网络的“引擎”：基于梯度的优化/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

                </div>
            

            

            

            
                <div class="article-nav">
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/03/03/03%EF%BD%9CMutex%EF%BC%9A4%E7%A7%8D%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%E5%A4%A7%E7%9B%98%E7%82%B9/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">03｜Mutex：4种易错场景大盘点</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">2.4　神经网络的“引擎”：基于梯度的优化</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BC%95%E6%93%8E%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96"><span class="nav-text">2-4
神经网络的“引擎”：基于梯度的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E6%95%B0"><span class="nav-text">2.4.1 导数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97%E7%9A%84%E5%AF%BC%E6%95%B0%E6%A2%AF%E5%BA%A6"><span class="nav-text">2.4.2 张量运算的导数：梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="nav-text">偏导数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">2.4.3 随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%B0%8F%E6%89%B9%E9%87%8Fsgd"><span class="nav-text">小批量随机梯度下降（小批量SGD）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd%E7%9A%84%E5%A4%9A%E7%A7%8D%E5%8F%98%E4%BD%93"><span class="nav-text">SGD的多种变体</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E9%87%8Fsgd"><span class="nav-text">动量SGD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="nav-text">2.4.4 链式求导：反向传播算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-text">01.链式法则</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E6%B1%82%E5%AF%BCchain-rule"><span class="nav-text">链式法则求导（Chain Rule）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE%E8%87%AA%E5%8A%A8%E5%88%92%E5%88%86"><span class="nav-text">02 用计算图自动划分</span></a></li></ol></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2022</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-regular fa-computer-classic"></i>&nbsp;&nbsp;<a href="/">The Redefine Team</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br> 
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v1.2.0</a>
        </div>
        
        
        
            <div id="start_time_div" style="display:none">
                2022/8/17 11:45:14
            </div>
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="unfolded-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="fa-regular fa-arrow-up"></i>
            </li>
        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="folded-tools-list">
        <li class="right-bottom-tools tool-toggle-show flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/menu-shrink.js"></script>

<script src="/js/tools/go-top-bottom.js"></script>

<script src="/js/tools/dark-light-toggle.js"></script>





    
<script src="/js/tools/code-block.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/layouts/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">





<div class="post-scripts pjax">
    
        
<script src="/js/tools/toc-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>




</body>
</html>
